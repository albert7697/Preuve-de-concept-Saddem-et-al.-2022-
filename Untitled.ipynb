{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d35faaa3-64fc-411c-8581-e24911ad796a",
   "metadata": {},
   "source": [
    "                                                            AMSS-CSC\n",
    "<p align=\"center\">\n",
    "                                                    <img src=\"./logo_ens.png\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9611d9f-23fc-4fb6-9dad-808707804a79",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <H1>Techniques et outils pour la preuve de concepts</H1>\n",
    "    Alberic Junior DASSI FEUSSOUO\n",
    "</div>\n",
    "\n",
    " <H3><i> Machine learning-based approach for online fault Diagnosis of Discrete Event System (Saddem et al. - 2022 )</H3> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de6d2b6-2b8d-4ea3-bd4a-113b298842a8",
   "metadata": {},
   "source": [
    "# 3. Modèle de l'algorithme d'apprentissage automatique pour le diagnostic\n",
    "Dans cette partie, nous allons construire l'algorithme de réseaux de neurones récurrents pour le diagnostic, lui fournir les données formatées dans <a  href =\"./preparation_des_donnees.ipynb\"> la section précédente</a>, l'entrainer, le tester et évaluer ses performances. <br>\n",
    "Nous rappelons que notre modèle est un classifieur multi-classes car au total, 8 classes sont considérées (C0 à C7). L’algorithme prend en entrée les données du système (au bon format) et renvoie en sortie sortie une distribution de probabilité sur les classes (encodées), activée grâce à la fonction <i><b>softmax</b></i>. L'architecture du modèle est visible sur la figure ci-dessous. qu’on peut le voir sur la figure ci-dessous (<i>section 3.6 de l’article</i>).<br>\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"./architecture_modele_RNN.png\">\n",
    "</div> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c908e9-fb9e-42b4-ac28-ec03d1008a36",
   "metadata": {},
   "source": [
    "# <H3> 3.1. Framework et bibliothèques nécessaires\n",
    "Un framework ou cadre de programmation est un environnement de travail qui permet de faciliter la\n",
    "création de modèles de Machine Learning en mettant à disposition différents systèmes avec différentes\n",
    "bibliothèques. L'article ne fait mention du Framework utilisé. Cependant, il s’agit d’une approche basée sur les réseaux de neurones récurrents avec LSTM (pour le traitement des données séquentielles), et l’utilisation des fonctions Softmax, Categorical Cross-Entropy, nous permettent de nous pencher sur les Framework TensorFlow et PyTorch. Pour notre travail, nous avons utilisé TensorFlow et son API keras. <br>\n",
    "De plus, afin d'évaluer les performances du modèle, nous avons utilisé les métriques de l'outil scikit-learn (pour dresser la matrice de confusion par exemple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338e68d2-2152-4da1-b9d3-2f682be92612",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73f0ce0-49cb-4459-9ad5-e05e58a5792c",
   "metadata": {},
   "source": [
    "# <H3>3.2.  Modèle\n",
    "Notre modèle de Machine Learning est concu de sorte a respecter l'architecture decrite a la section 3.6 de l'article. <br>\n",
    "* Attributs : Nombre de pas de temps, Nombre de classes, nombre de composante par pas de temps\n",
    "* Methodes : COnstruction de l'architecture du modele, la quantification de l'erreur du modele, l'entrainement, le test et l'evaluation sur les differents jeux de donnees, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f763abe-5045-4bc3-b7f3-5f5f0b76e85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelML:\n",
    "    \n",
    "    #Modèle RNN-LSTM basé sur la section 3.6\n",
    "    \n",
    "    def __init__(self, n_steps, n_features, n_classes):\n",
    "        #Constructeur\n",
    "        #Paramètres: \n",
    "        #n_steps (int)  : Nombre de pas de temps (N=50 dans l'article)\n",
    "        #n_features(int): Nombre de features par pas de temps\n",
    "        #n_classes(int) : Nombre de classes (8 : C0 à C7))\n",
    "\n",
    "        self.n_steps = n_steps\n",
    "        self.n_features = n_features\n",
    "        self.n_classes = n_classes\n",
    "        self.model = None\n",
    "        self.history = None\n",
    "        \n",
    "        print(f\"Initialisation du modèle LSTM\")\n",
    "        print(f\"  - Pas de temps (N) : {n_steps}\")\n",
    "        print(f\"  - Features : {n_features}\")\n",
    "        print(f\"  - Classes : {n_classes}\")\n",
    "    \n",
    "    def build_model(self, lstm_units=[128, 64], dropout=0.3):\n",
    "        \n",
    "        #La construction de l'architecture RNN-LSTM est inspirée \n",
    "        #de la Figure 3 de l'article :Input-LSTM layers-Dense-Softmax (8 neurones)\n",
    "        #Paramètres:\n",
    "        #lstm_units (list) : Nombre de neurones par couche LSTM\n",
    "        #dropout(float)    : c'est un taux qui permet d'éviter le surapprentissage\n",
    "\n",
    "        print(f\"\\nConstruction du modèle\")\n",
    "        print(f\" - Couches LSTM : {lstm_units}\")\n",
    "        print(f\" - Dropout : {dropout}\")\n",
    "        \n",
    "        model = keras.Sequential([\n",
    "            # Couche d'entrée\n",
    "            layers.Input(shape=(self.n_steps, self.n_features)),\n",
    "            \n",
    "            # Première couche LSTM (retourne les séquences)\n",
    "            layers.LSTM(lstm_units[0], return_sequences=True, name='LSTM_1'),\n",
    "            layers.Dropout(dropout),\n",
    "            \n",
    "            # Deuxième couche LSTM\n",
    "            layers.LSTM(lstm_units[1], return_sequences=False, name='LSTM_2'),\n",
    "            layers.Dropout(dropout),\n",
    "            \n",
    "            # Couche Dense intermédiaire\n",
    "            #ReLU = Rectified Linear Unit\n",
    "            #Permet de donner a la couche Dense la capacité d'apprendre des relations non linéaires\n",
    "            layers.Dense(32, activation='relu', name='Dense_1'),\n",
    "            layers.Dropout(dropout/2),\n",
    "           \n",
    "            # Couche de sortie avec Softmax (8 neurones pour 8 classes)\n",
    "            layers.Dense(self.n_classes, activation='softmax', name='Output')\n",
    "        ])\n",
    "        \n",
    "        self.model = model\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"ARCHITECTURE DU MODÈLE\")\n",
    "        print(\"=\"*70)\n",
    "        self.model.summary()\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def compile_model(self, learning_rate=0.001):\n",
    "        #Cette fonction permet de quantifier l'erreur entre les predictions du modele et les vraies classes\n",
    "        # grace a \"Categorical Cross-Entropy (CCE)\" tel que decrit dans l'article (section 4.2)\n",
    "        \n",
    "        #Learning rate = taux d'apprentissage\n",
    "        #C'est un hyperparametre qui definit la taille du pas que fait le modele lors de la mise a jour de ses poids.\n",
    "        \n",
    "\n",
    "\n",
    "        print(f\"\\nCompilation du modèle\")\n",
    "        print(f\" - Fonction de perte : Categorical Crossentropy (CCE)\")\n",
    "        print(f\" - Optimiseur : Adam\")\n",
    "        print(f\" - Learning rate : {learning_rate}\")\n",
    "        #Adam = Adaptive Moment Estimation.\n",
    "        #algorithme d’optimisation conçu pour mettre à jour les poids d’un réseau neuronal \n",
    "        #de manière plus intelligente que la descente de gradient classique.\n",
    "\n",
    "        self.model.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "            loss='sparse_categorical_crossentropy',  # CCE pour labels entiers\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        print(\" Modèle compilé\")\n",
    "    \n",
    "    def train(self, X_train, y_train, X_val, y_val, \n",
    "              epochs=100, batch_size=32, verbose=1):\n",
    "        \n",
    "        #Cette fonction permet l'entraînement du modèle avec validation\n",
    "        \n",
    "        #La section 3.2 indique l'usage du \"cross-validation\"\n",
    "        #Simple validation holdout\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"ENTRAÎNEMENT DU MODÈLE\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"Epochs : {epochs}\")\n",
    "        print(f\"Batch size : {batch_size}\")\n",
    "        print(f\"Échantillons d'entraînement : {len(X_train)}\")\n",
    "        print(f\"Échantillons de validation : {len(X_val)}\")\n",
    "        \n",
    "        # Callbacks pour améliorer l'entraînement\n",
    "        callbacks = [\n",
    "            # Arrêt précoce si pas d'amélioration\n",
    "            EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=15,\n",
    "                restore_best_weights=True,\n",
    "                verbose=1\n",
    "            ),\n",
    "            \n",
    "            # Réduction du learning rate\n",
    "            ReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                factor=0.5,\n",
    "                patience=5,\n",
    "                min_lr=1e-7,\n",
    "                verbose=1\n",
    "            ),\n",
    "            \n",
    "            # Sauvegarde du meilleur modèle\n",
    "            ModelCheckpoint(\n",
    "                'best_model.keras',\n",
    "                monitor='val_accuracy',\n",
    "                save_best_only=True,\n",
    "                verbose=1\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # Entraînement\n",
    "        self.history = self.model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=callbacks,\n",
    "            verbose=verbose\n",
    "        )\n",
    "        \n",
    "        print(\"\\n✓ Entraînement terminé\")\n",
    "        \n",
    "        return self.history\n",
    "    \n",
    "    def plot_training_history(self):\n",
    "\n",
    "        #Visualise l'historique d'entraînement\n",
    "        # Correspond aux Figures 8 et 9 de l'article\n",
    "        if self.history is None:\n",
    "            print(\"Aucun historique d'entraînement disponible\")\n",
    "            return\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # Graphique 1 : Loss (fonction d'erreur)\n",
    "        # Figure 8 de l'article : \"Evolution of the error function\"\n",
    "        ax1.plot(self.history.history['loss'], label='Train Loss', linewidth=2)\n",
    "        ax1.plot(self.history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "        ax1.set_xlabel('Epoch', fontsize=12)\n",
    "        ax1.set_ylabel('Loss (CCE)', fontsize=12)\n",
    "        ax1.set_title('Évolution de la fonction d\\'erreur', fontsize=14, fontweight='bold')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Graphique 2 : Accuracy\n",
    "        # Figure 9 de l'article : \"Evolution of the average accuracy\"\n",
    "        ax2.plot(self.history.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
    "        ax2.plot(self.history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "        ax2.set_xlabel('Epoch', fontsize=12)\n",
    "        ax2.set_ylabel('Accuracy', fontsize=12)\n",
    "        ax2.set_title('Évolution de l\\'accuracy moyenne', fontsize=14, fontweight='bold')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('training_history.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # Afficher les meilleurs résultats\n",
    "        best_epoch = np.argmax(self.history.history['val_accuracy'])\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"MEILLEURS RÉSULTATS\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"Meilleur epoch : {best_epoch + 1}\")\n",
    "        print(f\"Train Accuracy : {self.history.history['accuracy'][best_epoch]:.4f}\")\n",
    "        print(f\"Val Accuracy : {self.history.history['val_accuracy'][best_epoch]:.4f}\")\n",
    "        print(f\"Train Loss : {self.history.history['loss'][best_epoch]:.4f}\")\n",
    "        print(f\"Val Loss : {self.history.history['val_loss'][best_epoch]:.4f}\")\n",
    "    \n",
    "    def evaluate(self, X_test, y_test, class_names):\n",
    "\n",
    "        # Évaluation sur le jeu de test\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"ÉVALUATION SUR LE JEU DE TEST\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Prédictions\n",
    "        y_pred_probs = self.model.predict(X_test, verbose=0)\n",
    "        y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "        \n",
    "        # Accuracy globale\n",
    "        test_loss, test_acc = self.model.evaluate(X_test, y_test, verbose=0)\n",
    "        print(f\"\\nTest Loss : {test_loss:.4f}\")\n",
    "        print(f\"Test Accuracy : {test_acc:.4f}\")\n",
    "        \n",
    "        # Rapport de classification détaillé\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"RAPPORT DE CLASSIFICATION\")\n",
    "        print(\"=\"*70)\n",
    "        print(classification_report(y_test, y_pred, target_names=class_names))\n",
    "        \n",
    "        return y_pred, y_pred_probs\n",
    "    \n",
    "    def plot_confusion_matrix(self, y_test, y_pred, class_names):\n",
    "        \"\"\"\n",
    "        Matrice de confusion\n",
    "        Figure 10 de l'article : \"The average of normalized multi-class confusion matrix\"\n",
    "        \"\"\"\n",
    "        # Calculer la matrice de confusion\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        \n",
    "        # Visualisation\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
    "                    xticklabels=class_names, yticklabels=class_names,\n",
    "                    cbar_kws={'label': 'Proportion'})\n",
    "        plt.xlabel('Classe prédite', fontsize=12)\n",
    "        plt.ylabel('Classe réelle', fontsize=12)\n",
    "        plt.title('Matrice de confusion normalisée', fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        return cm, cm_normalized\n",
    "    \n",
    "    def calculate_metrics(self, cm):\n",
    "        \"\"\"\n",
    "        Calcule les métriques par classe\n",
    "        Section 3.6 : Precision et Recall\n",
    "        \n",
    "        Precision_i = TP_i / (TP_i + FP_i)\n",
    "        Recall_i = TP_i / (TP_i + FN_i)\n",
    "        \"\"\"\n",
    "        n_classes = cm.shape[0]\n",
    "        \n",
    "        precision = []\n",
    "        recall = []\n",
    "        \n",
    "        for i in range(n_classes):\n",
    "            TP = cm[i, i]\n",
    "            FP = cm[:, i].sum() - TP\n",
    "            FN = cm[i, :].sum() - TP\n",
    "            \n",
    "            prec = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "            rec = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "            \n",
    "            precision.append(prec)\n",
    "            recall.append(rec)\n",
    "        \n",
    "        return precision, recall\n",
    "    \n",
    "    def save_model(self, filepath='lstm_diagnostic_model.keras'):\n",
    "\n",
    "        #Sauvegarde le modèle entraîné\n",
    "\n",
    "        self.model.save(filepath)\n",
    "        print(f\"\\n Modèle sauvegardé : {filepath}\")\n",
    "    \n",
    "    def load_model(self, filepath='lstm_diagnostic_model.keras'):\n",
    "\n",
    "        #Charge un modèle pré-entraîné\n",
    "        self.model = keras.models.load_model(filepath)\n",
    "        print(f\" Modèle chargé : {filepath}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc54d80e-f5ee-49e7-a222-22ce9e8d9fbd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c50b63-6761-4e1c-a133-988953150ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"=\"*70)\n",
    "    print(\"MODÈLE RNN-LSTM POUR LE DIAGNOSTIC DE DÉFAUTS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # 1. Charger les données préparées\n",
    "    print(\"\\nChargement des données...\")\n",
    "    data = np.load('prepared_data.npz')\n",
    "    X_train = data['X_train']\n",
    "    X_val = data['X_val']\n",
    "    X_test = data['X_test']\n",
    "    y_train = data['y_train']\n",
    "    y_val = data['y_val']\n",
    "    y_test = data['y_test']\n",
    "    \n",
    "    print(f\" Données chargées\")\n",
    "    print(f\"  X_train shape : {X_train.shape}\")\n",
    "    print(f\"  y_train shape : {y_train.shape}\")\n",
    "    \n",
    "    # 2. Créer et construire le modèle\n",
    "    n_steps = X_train.shape[1]  # 50\n",
    "    n_features = X_train.shape[2]  # 4 (avec durées) ou 3 (sans)\n",
    "    n_classes = len(np.unique(y_train))  # 8\n",
    "    \n",
    "    model = ModelML(n_steps, n_features, n_classes)\n",
    "    model.build_model(lstm_units=[128, 64], dropout=0.3)\n",
    "    model.compile_model(learning_rate=0.001)\n",
    "    \n",
    "    # 3. Entraîner le modèle\n",
    "    history = model.train(\n",
    "        X_train, y_train,\n",
    "        X_val, y_val,\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # 4. Visualiser l'historique d'entraînement\n",
    "    model.plot_training_history()\n",
    "    \n",
    "    # 5. Évaluer sur le jeu de test\n",
    "    class_names = ['C0', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7']\n",
    "    y_pred, y_pred_probs = model.evaluate(X_test, y_test, class_names)\n",
    "    \n",
    "    # 6. Matrice de confusion\n",
    "    cm, cm_normalized = model.plot_confusion_matrix(y_test, y_pred, class_names)\n",
    "    \n",
    "    # 7. Métriques par classe\n",
    "    precision, recall = model.calculate_metrics(cm)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"MÉTRIQUES PAR CLASSE\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"{'Classe':<10} {'Precision':<12} {'Recall':<12}\")\n",
    "    print(\"-\" * 35)\n",
    "    for i, name in enumerate(class_names):\n",
    "        print(f\"{name:<10} {precision[i]:<12.4f} {recall[i]:<12.4f}\")\n",
    "    \n",
    "    # 8. Sauvegarder le modèle\n",
    "    model.save_model('lstm_diagnostic_model.keras')\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ENTRAÎNEMENT TERMINÉ AVEC SUCCÈS !\")\n",
    "    print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
